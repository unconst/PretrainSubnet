{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING]: failed to patch stdout/stderr for fork-safety: 'OutStream' object\n",
      "has no attribute 'buffer'\n",
      "[WARNING]: failed to reconfigure stdout/stderr with custom encoding error\n",
      "handler: 'OutStream' object has no attribute 'reconfigure'\n",
      "/Users/napoli/anaconda3/envs/311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import bittensor as bt\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Pull in training utils.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__is_set: {}\n",
      "accs_per_step: 3\n",
      "axon:\n",
      "  external_ip: null\n",
      "  external_port: null\n",
      "  ip: '[::]'\n",
      "  max_workers: 10\n",
      "  port: 8091\n",
      "bs: 1\n",
      "chain_endpoint: wss://test.finney.opentensor.ai\n",
      "config: null\n",
      "local: false\n",
      "logging:\n",
      "  debug: false\n",
      "  logging_dir: ~/.bittensor/miners\n",
      "  record_log: false\n",
      "  trace: false\n",
      "lr: 5.0e-05\n",
      "max_k: 1\n",
      "max_steps: 50000\n",
      "n_head: 12\n",
      "n_layer: 12\n",
      "netuid: 97\n",
      "num_warmup: 2000\n",
      "sl: 512\n",
      "steps_per_log: 1\n",
      "steps_per_sync: 10\n",
      "strict: false\n",
      "subtensor:\n",
      "  _mock: false\n",
      "  chain_endpoint: wss://entrypoint-finney.opentensor.ai:443\n",
      "  network: finney\n",
      "wallet:\n",
      "  hotkey: default\n",
      "  name: default\n",
      "  path: ~/.bittensor/wallets/\n",
      "wandb: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument( '--lr', type=float, default = 5e-5, help='Training learning rate.')\n",
    "    parser.add_argument( '--bs', type=int, default = 1, help='Training batch size')\n",
    "    parser.add_argument( '--sl', type=int, default = 512, help='Training sequence length')\n",
    "    parser.add_argument( '--n_head', type=int, default = 12, help='Model number of attention heads')\n",
    "    parser.add_argument( '--n_layer', type=int, default = 12, help='Number of gpt2 model layers')\n",
    "    parser.add_argument( '--local', action=\"store_true\", default = False, help='Turn on local training.')\n",
    "    parser.add_argument( '--wandb', action=\"store_true\", default = False, help='Turn on wandb')\n",
    "    parser.add_argument( '--max_k', type=int, default = 1, help='Max number of gradients to merge.')\n",
    "    parser.add_argument( '--max_steps', type=int, default = 50000, help='Max training steps.')\n",
    "    parser.add_argument( '--steps_per_log', type=int, default = 1, help='Number of steps per log.')\n",
    "    parser.add_argument( '--steps_per_sync', type=int, default = 10, help='Number of steps per chain sync.')\n",
    "    parser.add_argument( '--num_warmup', type=int, default = 2000, help='Scheduler warm up steps.')\n",
    "    parser.add_argument( '--accs_per_step', type=int, default= 3, help='Number of training accumulation steps.')\n",
    "    parser.add_argument( '--netuid', type = int, default = 97, help=\"The chain subnet uid.\" )\n",
    "    parser.add_argument( '--chain_endpoint', type = str, default = \"wss://test.finney.opentensor.ai\", help=\"The chain endpoint to connect with.\" )\n",
    "    bt.subtensor.add_args( parser )\n",
    "    bt.wallet.add_args( parser )\n",
    "    bt.axon.add_args( parser )\n",
    "    bt.logging.add_args( parser )\n",
    "    return bt.config( parser )\n",
    "\n",
    "config = parse_arguments()\n",
    "print (config)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model and tokenizer\n",
    "def setup_model_and_tokenizer():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel(GPT2Config(n_layer = config.n_layer, n_head = config.n_head)).to(device)\n",
    "    model.train()\n",
    "    return model, tokenizer, device\n",
    "\n",
    "model, tokenizer, device = setup_model_and_tokenizer()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataloader\n",
    "def load_dataloader():\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation = True, padding = \"max_length\", max_length = config.sl, return_tensors = \"pt\")\n",
    "    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T\", 'default', split='train', streaming=True)\n",
    "    dataset = dataset.shuffle(buffer_size = config.bs * 4, seed=42)\n",
    "    tokenized_dataset = dataset.map( tokenize_function, batched=True )\n",
    "    dataloader = DataLoader( tokenized_dataset, batch_size = config.bs)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = load_dataloader()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimized and scheduler\n",
    "optimizer = torch.optim.AdamW (model.parameters(), lr = config.lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.num_warmup, num_training_steps=config.max_steps)  # assuming total steps\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training loop\n",
    "step = 0\n",
    "accumulation_counter = 0\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch + 1}/{3}')\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        # Forward pass.\n",
    "        outputs = model(\n",
    "            input_ids = batch[\"input_ids\"].to(device), \n",
    "            attention_mask = batch[\"attention_mask\"].to(device),\n",
    "            labels = batch[\"input_ids\"].to(device)\n",
    "        ) \n",
    "        \n",
    "        # Backward pass\n",
    "        loss = outputs.loss / config.accs_per_step\n",
    "        loss.backward()\n",
    "\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_grads = {}\n",
    "compressed_sizes = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        element = param.grad.clone().detach().cpu()\n",
    "        norm, sign_xi_array = utils.compressor.compress(  element  )\n",
    "        compressed_grads[name] = bt.tensor( tensor = sign_xi_array )\n",
    "        compressed_sizes[name] = bt.tensor( tensor = norm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " grads = {}\n",
    "for name, compressed_grad in compressed_grads.items():\n",
    "    compressed_size = compressed_sizes[name]\n",
    "    grads[name] = compressor.decompress( sign_xi_array = compressed_grad.tensor(), norm = compressed_size.tensor() )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( sign_xi_array )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.quantization\n",
    "\n",
    "# Define a float tensor\n",
    "x = torch.rand(3, 3)\n",
    "\n",
    "# Quantize the tensor to 8 bits\n",
    "q_x = torch.quantization.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "\n",
    "# You can also dequantize it back to float\n",
    "dq_x = q_x.dequantize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
