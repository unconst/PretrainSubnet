{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tok( text: str )\n",
    "    tokenized = tokenizer.encode_plus( text, padding=\"max_length\", return_tensors=\"pt\" )\n",
    "\n",
    "\n",
    "    buffer = []\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        print(\"full text tokenized:\", tokenizer.encode_plus(text))\n",
    "        print(\"initial text:\", text)\n",
    "        tokenized_text_full = tokenizer.encode_plus(text)\n",
    "        len(\"len input_ids full text:\", tokenized_text_full['input_ids'])\n",
    "\n",
    "        tokenized_parts = []\n",
    "        input_ids = tokenized_text_full['input_ids']\n",
    "        attention = tokenized_text_full['attention_mask']\n",
    "\n",
    "        ids_list = []\n",
    "        attention_list = []\n",
    "        for i in range(0, len(input_ids), max_seq_length):\n",
    "            end = min(i + max_seq_length, len(input_ids))\n",
    "            ids = input_ids[i:end]\n",
    "            attention = attention[i:end]\n",
    "            # If the tensor is shorter than SEQUENCE_LENGTH, pad it\n",
    "            if len(ids) < max_seq_length:\n",
    "                ids += [tokenizer.pad_token_id] * (max_seq_length - len(ids))\n",
    "                attention += [0] * (max_seq_length - len(attention))\n",
    "            ids_list.append(ids)\n",
    "            attention_list.append(attention)\n",
    "\n",
    "        import pdb; pdb.set_trace()\n",
    "        # If a single text item is longer than max_seq_length,\n",
    "        # we split it into multiple parts.\n",
    "        while len(text) > 0:\n",
    "            print(\"\\ntext in while:\", text)\n",
    "            tokenized = tokenize_single_seq(text)\n",
    "            # xx = tokenized.input_ids.split(max_seq_length)\n",
    "            # yy = tokenized.attention_mask = tokenized.attention_mask.split(max_seq_length)\n",
    "            \n",
    "            # tokenized.split() [:min(max_seq_length, len(text))]\n",
    "            print(\"\\ntokenized:\", tokenized)\n",
    "            print(\"len tokenized ids:\", len(tokenized['input_ids'][0]))\n",
    "            tokenized_parts.append(tokenized)\n",
    "            print(\"\\ntokenized_parts:\", tokenized_parts)\n",
    "            text = text[tokenized['input_ids'].shape[1]-1:]  # reduce the text length by the number of tokens we just encoded\n",
    "            print(\"\\ntext after reduction:\", text)\n",
    "\n",
    "        buffer.extend(tokenized_parts)\n",
    "        print(\"\\nextended buffer:\", buffer)\n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        # If buffer size exceeds batch size, yield the batch and reset buffer.\n",
    "        while len(buffer) >= batch_size:\n",
    "            yield {\n",
    "                \"input_ids\": torch.cat([t[\"input_ids\"] for t in buffer[:batch_size]], dim=0),\n",
    "                \"attention_mask\": torch.cat([t[\"attention_mask\"] for t in buffer[:batch_size]], dim=0),\n",
    "            }\n",
    "            buffer = buffer[batch_size:]\n",
    "\n",
    "\n",
    "sequence_length=5\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_data_generator = _tokenize_data(\n",
    "    dataset, \n",
    "    tokenizer=tokenizer, \n",
    "    max_seq_length=sequence_length\n",
    ")\n",
    "\n",
    "for batch in tokenized_data_generator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/napoli/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1-data_dir=test/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1641.82it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/napoli/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1-data_dir=test/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1562.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', 'test')\n",
    "text = ''.join(dataset['test']['text'])\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "SEQUENCE_LENGTH = 512  # for example\n",
    "BATCH_SIZE = 10\n",
    "def encode(text):\n",
    "    input_ids_buffer = []\n",
    "    attention_buffer = []\n",
    "    encoding  = tokenizer( text )\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    for i in range(0, len(input_ids), SEQUENCE_LENGTH):\n",
    "        end = min(i + SEQUENCE_LENGTH, len(input_ids))\n",
    "        tensor = input_ids[i:end]\n",
    "        attention = attention_mask[i:end]\n",
    "        # If the tensor is shorter than SEQUENCE_LENGTH, pad it\n",
    "        if len(tensor) < SEQUENCE_LENGTH:\n",
    "            tensor += [tokenizer.pad_token_id] * (SEQUENCE_LENGTH - len(tensor))\n",
    "            attention += [0] * (SEQUENCE_LENGTH - len(attention))\n",
    "        input_ids_buffer.append(tensor)\n",
    "        attention_buffer.append(attention)\n",
    "        while len(input_ids_buffer) >= BATCH_SIZE:\n",
    "            batch = {\n",
    "                'input_ids': input_ids_buffer[:BATCH_SIZE].stack(0),\n",
    "                'attention_mask': attention_buffer[:BATCH_SIZE].stack(0),\n",
    "            }\n",
    "            return batch\n",
    "            # print (batch)\n",
    "            # yield batch\n",
    "            # input_ids_buffer = input_ids_buffer[BATCH_SIZE:]\n",
    "            # attention_buffer = attention_buffer[BATCH_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in encode( text[:1000] ): print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode( text[:1000] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283287"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( encoding['input_ids'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
